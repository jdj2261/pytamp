{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pykin.utils import plot_utils as p_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearplot_with_confidence(x_list, error_list, label, marker=\"\", color='r'):\n",
    "    mean_list = np.squeeze(np.mean(error_list,axis=0))\n",
    "    std_list = np.std(error_list,axis=0)\n",
    "    a = 0.1\n",
    "    plt.fill_between(x_list, mean_list-a*std_list,mean_list+a*std_list, alpha=0.13, color=color)\n",
    "    plt.plot(x_list,mean_list,label=label,marker=marker, ms=11, c=color, markevery=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "budgets = 100\n",
    "end_num = 0\n",
    "algo = \"bai_perturb\"\n",
    "filename = f'benchmark3_result/benchmark3_test_algo({algo})_budget({budgets})_seed({seed})_{end_num}.npy'\n",
    "\n",
    "perturb_final_level_1_values = []\n",
    "perturb_final_level_2_values = []\n",
    "perturb_final_pnp_all_joint_paths = []\n",
    "\n",
    "with open(filename,'rb') as f:\n",
    "    data_for_seed = np.load(f, allow_pickle=True)\n",
    "    perturb_final_level_1_values.append(data_for_seed['level_1_values'])\n",
    "    perturb_final_level_2_values.append(data_for_seed['level_2_values'])\n",
    "    perturb_final_pnp_all_joint_paths.append(data_for_seed['pnp_all_joint_paths'])\n",
    "\n",
    "fig, ax = p_utils.init_2d_figure(\"test\")\n",
    "mean_result = np.mean(perturb_final_level_1_values[0], axis=0)\n",
    "p_utils.plot_values(\n",
    "    ax,\n",
    "    mean_result, \n",
    "    label=\"mean\", \n",
    "    title=\"Benchamrk1_Level_1\", \n",
    "    save_dir_name='benchmark3_result', \n",
    "    is_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2\n",
    "budgets = 100\n",
    "end_num = 0\n",
    "algo = \"uct\"\n",
    "filename = f'benchmark3_result/benchmark3_test_algo({algo})_budget({budgets})_seed({seed})_{end_num}.npy'\n",
    "\n",
    "uct_final_level_1_values = []\n",
    "uct_final_level_2_values = []\n",
    "uct_final_pnp_all_joint_paths = []\n",
    "\n",
    "with open(filename,'rb') as f:\n",
    "    data_for_seed = np.load(f, allow_pickle=True)\n",
    "    uct_final_level_1_values.append(data_for_seed['level_1_values'])\n",
    "    uct_final_level_2_values.append(data_for_seed['level_2_values'])\n",
    "    uct_final_pnp_all_joint_paths.append(data_for_seed['pnp_all_joint_paths'])\n",
    "\n",
    "fig, ax = p_utils.init_2d_figure(\"test\")\n",
    "mean_result = np.mean(uct_final_level_1_values[0], axis=0)\n",
    "p_utils.plot_values(\n",
    "    ax,\n",
    "    mean_result, \n",
    "    label=\"mean\", \n",
    "    title=\"Benchamrk1_Level_1\", \n",
    "    save_dir_name='benchmark3_result', \n",
    "    is_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2\n",
    "budgets = 100\n",
    "end_num = 0\n",
    "algo = \"random\"\n",
    "filename = f'benchmark3_result/benchmark3_test_algo({algo})_budget({budgets})_seed({seed})_{end_num}.npy'\n",
    "\n",
    "random_final_level_1_values = []\n",
    "random_final_level_2_values = []\n",
    "random_final_pnp_all_joint_paths = []\n",
    "\n",
    "with open(filename,'rb') as f:\n",
    "    data_for_seed = np.load(f, allow_pickle=True)\n",
    "    random_final_level_1_values.append(data_for_seed['level_1_values'])\n",
    "    random_final_level_2_values.append(data_for_seed['level_2_values'])\n",
    "    random_final_pnp_all_joint_paths.append(data_for_seed['pnp_all_joint_paths'])\n",
    "\n",
    "fig, ax = p_utils.init_2d_figure(\"test\")\n",
    "mean_result = np.mean(random_final_level_1_values[0], axis=0)\n",
    "p_utils.plot_values(\n",
    "    ax,\n",
    "    mean_result, \n",
    "    label=\"mean\", \n",
    "    title=\"Benchamrk1_Level_1\", \n",
    "    save_dir_name='benchmark3_result', \n",
    "    is_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "budgets = 100\n",
    "end_num = 0\n",
    "algo = \"bai_ucb\"\n",
    "filename = f'benchmark3_result/benchmark3_test_algo({algo})_budget({budgets})_seed({seed})_{end_num}.npy'\n",
    "\n",
    "bai_ucb_final_level_1_values = []\n",
    "bai_ucb_final_level_2_values = []\n",
    "bai_ucb_final_pnp_all_joint_paths = []\n",
    "\n",
    "with open(filename,'rb') as f:\n",
    "    data_for_seed = np.load(f, allow_pickle=True)\n",
    "    bai_ucb_final_level_1_values.append(data_for_seed['level_1_values'])\n",
    "    bai_ucb_final_level_2_values.append(data_for_seed['level_2_values'])\n",
    "    bai_ucb_final_pnp_all_joint_paths.append(data_for_seed['pnp_all_joint_paths'])\n",
    "\n",
    "fig, ax = p_utils.init_2d_figure(\"test\")\n",
    "mean_result = np.mean(bai_ucb_final_level_1_values[0], axis=0)\n",
    "p_utils.plot_values(\n",
    "    ax,\n",
    "    mean_result, \n",
    "    label=\"mean\", \n",
    "    title=\"Benchamrk1_Level_1\", \n",
    "    save_dir_name='benchmark3_result', \n",
    "    is_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = 10*np.arange(len(perturb_final_level_1_values[0][0]))\n",
    "linearplot_with_confidence(x_list, perturb_final_level_1_values[0], \"Perturb\", \">\", \"m\")\n",
    "linearplot_with_confidence(x_list, uct_final_level_1_values[0], \"uct\",\"o\", \"r\")\n",
    "linearplot_with_confidence(x_list, random_final_level_1_values[0], \"random\",\"^\", \"b\")\n",
    "linearplot_with_confidence(x_list, bai_ucb_final_level_1_values[0], \"bai-ucb\",\"o\", \"y\")\n",
    "# plt.xlim([np.min(x_list),np.max(x_list)])\n",
    "plt.xlabel(\"Number of Data\",fontsize=25)\n",
    "plt.ylabel(\"Reward\",fontsize=25)\n",
    "\n",
    "plt.xticks(x_list[::25],x_list[::25],fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size' : 23})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Result Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds=5\n",
    "budgets = 100\n",
    "end_num = 0\n",
    "algos = [\"bai_perturb\", \"uct\", \"random\", \"bai_ucb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_level_1_values = [[] for _ in range(len(algos))]\n",
    "final_level_2_values = [[] for _ in range(len(algos))]\n",
    "final_pnp_all_joint_paths = [[] for _ in range(len(algos))]\n",
    "for idx, algo in enumerate(algos):\n",
    "    for seed in range(1, n_seeds+1):\n",
    "        filename = 'benchmark3_result/benchmark3_test_algo({:})_budget({:})_seed({:})_{}.npy'.format(algo, budgets, seed, end_num)\n",
    "        try:\n",
    "            with open(filename,'rb') as f:\n",
    "                data_for_seed = np.load(f, allow_pickle=True)\n",
    "                final_level_1_values[idx].append(data_for_seed['level_1_values'])\n",
    "                final_level_2_values[idx].append(data_for_seed['level_2_values'])\n",
    "                final_pnp_all_joint_paths[idx].append(data_for_seed['pnp_all_joint_paths'])\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1_results = [[] for _ in range(len(algos))]\n",
    "for idx, algo in enumerate(algos):\n",
    "    for i in range(n_seeds):\n",
    "        level_1_results[idx].append(np.mean(final_level_1_values[idx][i], axis=0))\n",
    "\n",
    "# result = np.mean(results, axis=0)\n",
    "x_list = 10*np.arange(len(level_1_results[0][0]))\n",
    "plt.xlim([np.min(x_list),np.max(x_list)])\n",
    "linearplot_with_confidence(x_list, level_1_results[0], \"Perturb\", \">\", \"m\")\n",
    "linearplot_with_confidence(x_list, level_1_results[1], \"uct\", \"^\", \"r\")\n",
    "linearplot_with_confidence(x_list, level_1_results[2], \"random\", \"v\", \"c\")\n",
    "linearplot_with_confidence(x_list, level_1_results[3], \"bai_ucb\", \"o\", \"g\")\n",
    "plt.xlabel(\"Number of Data\",fontsize=25)\n",
    "plt.ylabel(\"Reward\",fontsize=25)\n",
    "\n",
    "plt.xticks(x_list[::25],x_list[::25],fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size' : 23})\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
